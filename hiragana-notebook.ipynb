{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba348f3",
   "metadata": {},
   "source": [
    "# Hiragana convolutional network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ebe08a",
   "metadata": {},
   "source": [
    "The Hiragana datatset is composed of the 46 japanese characters called hiragana. Each folder is named after the hiragana name and contains 100 jpg images of handwritten hiragana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7dcbd389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d645d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "358cdc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f9eb40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad70b78",
   "metadata": {},
   "source": [
    "## Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31443939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388f23e1",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5765b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"hiragana-dataset\"\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 64\n",
    "NUM_CLASSES = 46\n",
    "EPOCHS = 15\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "667cbe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e75ac3a",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e629c40c",
   "metadata": {},
   "source": [
    "Data augmentation consist in small processing steps like small rotation, scaling etc. Be careful to not do any horizontal flip as it might change the meaning of the hiragana. the hiragana images are already inn black and white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b322f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform =  transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1),\n",
    "        scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2366553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,), (0.5,))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb0bfc6",
   "metadata": {},
   "source": [
    "## Load the hiragana dataset with Imagefolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7155df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['aa', 'chi', 'ee', 'fu', 'ha', 'he', 'hi', 'ho', 'ii', 'ka', 'ke', 'ki', 'ko', 'ku', 'ma', 'me', 'mi', 'mo', 'mu', 'na', 'ne', 'ni', 'nn', 'no', 'nu', 'oo', 'ra', 're', 'ri', 'ro', 'ru', 'sa', 'se', 'shi', 'so', 'su', 'ta', 'te', 'to', 'tsu', 'uu', 'wa', 'wo', 'ya', 'yo', 'yu']\n",
      "Number of images: 4600\n"
     ]
    }
   ],
   "source": [
    "full_dataset = datasets.ImageFolder(root=DATA_DIR, transform=train_transform)\n",
    "print(\"Classes:\", full_dataset.classes)\n",
    "print(\"Number of images:\", len(full_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbed5dd",
   "metadata": {},
   "source": [
    "## Stratified Train/Validation split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960194e8",
   "metadata": {},
   "source": [
    "I want to make sure I have all the classes represented in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "666e1417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(dataset, val_ratio = 0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    class_indices = defaultdict(list)\n",
    "    \n",
    "    for idx, (_,label) in enumerate(dataset.samples):\n",
    "        class_indices[label].append(idx)\n",
    "        \n",
    "    train_idx, val_idx = [], []\n",
    "    for label, indices in class_indices.items():\n",
    "        np.random.shuffle(indices)\n",
    "        split = int(len(indices) * (1 - val_ratio))\n",
    "        train_idx.extend(indices[:split])\n",
    "        val_idx.extend(indices[split:])\n",
    "        \n",
    "    return train_idx, val_idx\n",
    "\n",
    "train_idx, val_idx = stratified_split(full_dataset, val_ratio=0.2)\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "val_dataset = Subset(datasets.ImageFolder(DATA_DIR, transform=val_transform),\n",
    "                     val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "319de1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3680\n",
      "Validation size: 920\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Validation size:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84611a6b",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7200e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "775eb1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 64, 64])\n",
      "tensor([35,  9, 42, 27, 19])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape) # e.g., torch.Size([32, 1, 64, 64])\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b3d3ae",
   "metadata": {},
   "source": [
    "## Hiragana CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789d26b4",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32e02afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiraganaCNN(nn.Module):\n",
    "    def __init__(self, num_classes=46):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 8 * 8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312f294b",
   "metadata": {},
   "source": [
    "## Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e63ee589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HiraganaCNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=8192, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=46, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = HiraganaCNN(NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR) # LR = 1e-3\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13994872",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d28c411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:32<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15]Loss: 452.175 | Val Acc: 18.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:29<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15]Loss: 328.800 | Val Acc: 76.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:30<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/15]Loss: 246.308 | Val Acc: 91.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:29<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/15]Loss: 211.531 | Val Acc: 97.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:29<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/15]Loss: 188.690 | Val Acc: 99.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:29<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/15]Loss: 179.153 | Val Acc: 97.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:30<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/15]Loss: 171.453 | Val Acc: 93.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:30<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/15]Loss: 165.542 | Val Acc: 99.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:31<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/15]Loss: 159.225 | Val Acc: 99.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:31<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/15]Loss: 155.724 | Val Acc: 99.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:31<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/15]Loss: 148.018 | Val Acc: 99.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:32<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/15]Loss: 154.770 | Val Acc: 99.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:30<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/15]Loss: 146.109 | Val Acc: 99.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:30<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/15]Loss: 147.287 | Val Acc: 99.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:29<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15]Loss: 140.494 | Val Acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss=0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    # Validation part\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}]\" f\"Loss: {train_loss:.3f} | Val Acc: {acc:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c8a7a4",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b0184",
   "metadata": {},
   "source": [
    "### Class mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "684705c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'aa', 1: 'chi', 2: 'ee', 3: 'fu', 4: 'ha', 5: 'he', 6: 'hi', 7: 'ho', 8: 'ii', 9: 'ka', 10: 'ke', 11: 'ki', 12: 'ko', 13: 'ku', 14: 'ma', 15: 'me', 16: 'mi', 17: 'mo', 18: 'mu', 19: 'na', 20: 'ne', 21: 'ni', 22: 'nn', 23: 'no', 24: 'nu', 25: 'oo', 26: 'ra', 27: 're', 28: 'ri', 29: 'ro', 30: 'ru', 31: 'sa', 32: 'se', 33: 'shi', 34: 'so', 35: 'su', 36: 'ta', 37: 'te', 38: 'to', 39: 'tsu', 40: 'uu', 41: 'wa', 42: 'wo', 43: 'ya', 44: 'yo', 45: 'yu'}\n"
     ]
    }
   ],
   "source": [
    "idx_to_class = {v: k for k, v in full_dataset.class_to_idx.items()}\n",
    "print(idx_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8b15aa",
   "metadata": {},
   "source": [
    "### Predict one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5b2fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96a8faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = val_transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "        pred = torch.argmax(output, 1).item()\n",
    "        \n",
    "    return idx_to_class[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "84314524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n"
     ]
    }
   ],
   "source": [
    "print(predict_image(\"hiragana-dataset/aa/drawing_20250805_081540.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177b406",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "840e1209",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"hiragana_cnn.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
